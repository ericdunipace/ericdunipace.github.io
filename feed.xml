<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ericdunipace.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ericdunipace.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-01T02:53:03+00:00</updated><id>https://ericdunipace.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Eric A. Dunipace — MD/PhD </subtitle><entry><title type="html">WpProj hits CRAN!</title><link href="https://ericdunipace.github.io/blog/2024/WpProj-Package-Is-Live/" rel="alternate" type="text/html" title="WpProj hits CRAN!"/><published>2024-02-02T00:00:00+00:00</published><updated>2024-02-02T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2024/WpProj-Package-Is-Live</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2024/WpProj-Package-Is-Live/"><![CDATA[<p><strong>Edit (1/30/2025):</strong> *This package was archived on CRAN in November 2024 due to a package it depended on being archived. Working on getting it back up ASAP *</p> <hr/> <p>This package was quite a mess. It was the first one I’d ever decided to do and filled with na”ivet’e and hubris, I thought it would be a good idea to make a package to load all my functions. It definitely took a long time—especially because a lot of the code was adapting someone else’s C++ code to run for my purpose!—but I think it’s not too bad. There are some places I see that would be obvious places to improve the models but required more work than I currently have time to put in, unfortunately.</p> <h2 id="what-does-the-package-do">What does the package do?</h2> <p>Basically, this package was my round about way of re-inventing \(L_p\) regression. Only slightly kidding. The basic idea is as follows.</p> <p>Say you have some arbitrarily complex model, \(f\), that takes covariates \(x\), and generates a set of predictions, call them \(\hat{y}\in \mathcal{Y}\), that follow a distribution \(\mu\), with an empirical counterpart \(\hat{\mu}\). Now say this model is really hard to interpret how the \(x\)’s affect the predictions. Let’s say we have another class of models, \(g\), that are easy to interpret. Say these are linear models that typically have the form \(x\beta\). We will denote predictions from this model as \(\hat{\eta}\) and let them have some unspecified distribution \(\nu\) and empirical counterpart \(\hat{\nu}\). (Note that since the \(x\)’s are considered fixed, the distribution is actually coming from the \(\beta\)’s, i.e. \(x\beta \sim \nu\))</p> <p>It’d be nice if we could use this set of interpretable models from \(g\) to help us understand what’s happening in \(f\). Ideally, these models in \(g\) would be close in some sense to \(f\). We desire</p> <ol> <li>Fidelity: predictions from our \(g\) models should be close to \(f\)</li> <li>Interpretability: we should be able to understand what’s happening in \(g\). This also implies our models can’t have too many coefficients.</li> </ol> <p>Let’s address each of these in turn. For 1, we need some way of ensuring predictive distributions are close to one another. One such metric is the \(p\)-Wasserstein distance, defined as</p> \[W_p(\hat{\mu}, \hat{\nu}) = \left( \inf_\pi \int \|\hat{y} - \hat{\nu}\|^p \pi(d\hat{y}, d\hat{\nu}) \right)^{1/p}.\] <p>Then we seek to minimize \(\inf_\hat{\nu} W_p(\hat{\mu}, \hat{\nu})^p.\)</p> <p>Now, for 2. Since the parameter space of \(\hat{\nu}\) could be quite large if the dimensionality of \(x\) is large, then we might not still have an interpretable model—I’d argue that a 1000 covariate regression model is <strong>not</strong>, in fact, interpretable!</p> <p>Going back to our minimization problem, we want to add some kind of penalty for large parameter distributions</p> \[\inf_\hat{\nu} W_p(\hat{\mu}, \hat{\nu})^p + P_\lambda (\hat{\nu}).\] <p>We should note that \(\hat{\nu} = x \hat{\beta}\). So, we need someway of reducing the dimensionality of the \(x\)’s. But fortunately, for linear models there’s a decades old method of doing just that!</p> <p>Rather than focusing on the \(x\)’s, we focus on reducing the dimensions of \(\beta\) using a penalty like the group Lasso:</p> \[P_\lambda (\beta) = \lambda \| \beta^{(1)} \|_2 + \lambda \|\beta^{(2)}\|_2 + ...\] <p>Finally, if we let the number of atoms in empricical distributions be equal, then the problem will reduce to</p> \[\inf_\beta \sum_i \left\| \hat{y}_i - x \beta_i\right\|_p^p + \lambda \sum_j \left \|\beta^{(j)}\right\|_2.\] <p>Cool!</p> <h2 id="lets-see-an-example">Let’s see an example</h2> <p>Ok, say we have some covariate data, \(X\), and an outcome, \(Y\). In many cases, the size of \(X\) can be quite large—in the 100s or 1000s. The question of then how to interpret this model can be tough: what covariates do we focus on? Moreover, the model itself may not be interpretable to begin with, such as from a Gaussian Process, a neural network, etc.</p> <h3 id="estimating-a-data-model">Estimating a data model</h3> <p>For exposition, we will generate our data from a hard to interpret, non-linear model and then fit a Bayesian Gaussian Process regression to estimate the response surface. The set-up will be somewhat complicated but it’s basically to generate complicated data and fit a complicated model.</p> <p>First, let’s assume our data is drawn from the following distributions. Let \(p = 10\) and \(n = 1000\). Take</p> \[X_j \sim \mathcal{N} (0, \mathbb{I}_p)\] <p>and</p> \[Y_j = f(X_j) + \epsilon_j\] <p>where \(\epsilon_{1:n} \sim \mathcal{N}(0,1)\). The function $f$ is defined as</p> \[f(X_j) = \alpha_0 + \sum_{k=1}^p \alpha_k X_{j,k} + \sum_{k=1, k' &gt;k }^p \delta_{k,k'} X_{j,k} X_{j,k'}.\] <p>We can generate the parameters of this generating function by taking \(\alpha_0, \alpha, \delta \sim \mathcal{N}(0,1)\) and \(\epsilon_{1:n} \sim \mathcal{N}(0,1)\)</p> <p>Then, assume for some reason we know the true model (this is so we can run all of the methods for our package). And we will fit a Bayesian regression model directly on this model.</p> <p>We can assume normal priors on the coefficients (which will be the same as the data generating process above), and a half-normal prior on the standard deviation: \(\sigma \sim \mathcal{N}^{+}(0,1)\)</p> <p>We can then fit this model with the following <code class="language-plaintext highlighter-rouge">R</code> code:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">rstan</span><span class="p">)</span><span class="w">

</span><span class="c1"># Simulated Data</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">42</span><span class="p">)</span><span class="w">
</span><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="o">^</span><span class="m">10</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">10</span><span class="w">

</span><span class="c1"># parameters</span><span class="w">
</span><span class="n">alpha_delta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">p</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">choose</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">alpha_0</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="w">

</span><span class="c1"># data</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">matrix</span><span class="p">(</span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">p</span><span class="p">),</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w">
</span><span class="n">mm</span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">.</span><span class="o">*</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w">
</span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">mm</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">alpha_delta</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">alpha_0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">))</span><span class="w">

</span><span class="n">code</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s1">'
data {
  int N;
  int P;
  vector[N] Y;
  matrix[N,P] X;
}
parameters {
  vector[P] alpha;
  real&lt;lower=0&gt; sigma;
  real alpha_0;
}
model {
  vector[N] mu_raw = X * beta + beta_0;
  
  alpha0 ~ normal(0,1);
  alpha ~ normal(0,1);
  sigma ~ normal(0,1);
  Y ~ normal(mu_raw, sigma);
}
generated quantities {
  vector[N] mu = X * beta + beta_0;
}
'</span><span class="w">

</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">stan</span><span class="p">(</span><span class="n">model_code</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">code</span><span class="p">,</span><span class="w"> 
            </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">P</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ncol</span><span class="p">(</span><span class="n">mm</span><span class="p">),</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mm</span><span class="p">),</span><span class="w">
            </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">chains</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h3 id="interpretable-model">Interpretable model</h3> <p>We can estimate an interpretable model, which essentially ammounts to fitting regression to the samples. How we do this can vary for our methods. Since we <em>know</em> the true model, we may simply seek to turn the covariates on our off and get our set of interpretable coefficients. Alternatively, we may want to find an approximate model with new coefficients. We can do both in the framework briefly described above. The basic idea is to find coefficients such that \(\hat{\nu} = f_\beta(X)\) is as close as possible to \(\hat{\mu}\). However, it is important to consider <em>what</em> we want to interpret.</p> <p>We could want to know how the model roughly functions globally, or which covariates could be the most important, but we may also want to know which covariates are driving the prediction for a single individual and consider the most important ones.</p> <p>Let’s say we’re interested in the 5th individual in our data. We can pul their data</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_mm</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="n">mm</span><span class="p">)</span><span class="w">
</span><span class="n">X_test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">X_mm</span><span class="p">[</span><span class="m">5</span><span class="p">,]</span><span class="w">
</span></code></pre></div></div> <p>And then we can run our interpretable models. We first run our set that can get interpretable models for a single data point–i.e., by simply turning coefficients on or off that best predict the data. The details are explained more fully in the paper, but basically for \(W_2\) distances, we can represent the problem as a binary program:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">WpProj</span><span class="p">)</span><span class="w">

</span><span class="c1"># get parameters</span><span class="w">
</span><span class="n">mu</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">extract</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">pars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"mu"</span><span class="p">)</span><span class="o">$</span><span class="n">mu</span><span class="w">
</span><span class="n">beta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">do.call</span><span class="p">(</span><span class="s2">"cbind"</span><span class="p">,</span><span class="w"> </span><span class="n">extract</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">pars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"beta_0"</span><span class="p">,</span><span class="s2">"beta"</span><span class="p">)))</span><span class="w">

</span><span class="c1"># get prediction</span><span class="w">
</span><span class="n">mu_test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mu</span><span class="p">[,</span><span class="m">5</span><span class="p">]</span><span class="w">
  
</span><span class="c1"># get interpretable models            </span><span class="w">
</span><span class="n">bp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">WpProj</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mu_test</span><span class="p">,</span><span class="w"> </span><span class="n">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w">
             </span><span class="n">power</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binary program"</span><span class="p">,</span><span class="w">
             </span><span class="n">solver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"ecos"</span><span class="p">,</span><span class="w">
             </span><span class="n">options</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">display.progress</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span><span class="w">

</span><span class="n">approx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">WpProj</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">X_test</span><span class="p">,</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mu_test</span><span class="p">,</span><span class="w"> </span><span class="n">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w">
                 </span><span class="n">power</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"binary program"</span><span class="p">,</span><span class="w">
                 </span><span class="n">solver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lasso"</span><span class="p">,</span><span class="w">
             </span><span class="n">options</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">display.progress</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <p>We can also fit a model that simply finds a lasso regression closest to \(\hat{\mu}\). To do so, we need to create a pseudo neighborhood round our point of interest</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">mvtnorm</span><span class="p">)</span><span class="w">

</span><span class="n">pp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ncol</span><span class="p">(</span><span class="n">mm</span><span class="p">)</span><span class="w"> 

</span><span class="c1"># generate the neighborhood arround the point</span><span class="w">
</span><span class="n">X_neigh</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mvtnorm</span><span class="o">::</span><span class="n">rmvnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">X_test</span><span class="p">[</span><span class="m">-1</span><span class="p">],</span><span class="w"> </span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cov</span><span class="p">(</span><span class="n">X_mm</span><span class="p">[,</span><span class="m">-1</span><span class="p">])</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="w">

</span><span class="c1"># get predictions for the neighborhood</span><span class="w">
</span><span class="n">mu_neigh</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="n">X_neigh</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="w">

</span><span class="c1"># get projection</span><span class="w">
</span><span class="n">proj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">WpProj</span><span class="p">(</span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="n">X_neigh</span><span class="p">),</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mu_neigh</span><span class="p">,</span><span class="w"> 
               </span><span class="n">theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w">
               </span><span class="n">power</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"L1"</span><span class="p">,</span><span class="w">
               </span><span class="n">solver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lasso"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <h3 id="performance-evaluation">Performance Evaluation</h3> <p>For our individual, they have an average predicted value of -6.7314591.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check WpR2</span><span class="w">
</span><span class="n">rp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">WpProj</span><span class="o">::</span><span class="n">ridgePlot</span><span class="p">(</span><span class="n">fit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="s2">"BP"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bp</span><span class="p">,</span><span class="w">
                                    </span><span class="s2">"approxBP"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">approx</span><span class="p">,</span><span class="w">
                                    </span><span class="s2">"projection"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">proj</span><span class="p">),</span><span class="w">
                          </span><span class="n">full</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mu_test</span><span class="w">
                          </span><span class="p">)</span><span class="w">

</span><span class="n">print</span><span class="p">(</span><span class="n">rp</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p><img src="_posts/2024-02-02-WpProj-Package-Is-Live_files/figure-gfm/unnamed-chunk-5-1.png" alt="Full-width image"/></p> <p>We can see that the interpretable models do a better job of predicting as covariates are added but we might need more than just 10 covariates to really do a good job here.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check WpR2</span><span class="w">
</span><span class="n">wpr2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">WpProj</span><span class="o">::</span><span class="n">WPR2</span><span class="p">(</span><span class="n">predictions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mu_test</span><span class="p">,</span><span class="w">
             </span><span class="n">projected_model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="s2">"BP"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bp</span><span class="p">,</span><span class="w">
                                    </span><span class="s2">"approxBP"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">approx</span><span class="p">,</span><span class="w">
                                    </span><span class="s2">"projection"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">proj</span><span class="p">),</span><span class="w">
             </span><span class="n">base</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1000</span><span class="p">))</span><span class="w">

</span><span class="n">plot</span><span class="p">(</span><span class="n">wpr2</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p><img src="_posts/2024-02-02-WpProj-Package-Is-Live_files/figure-gfm/unnamed-chunk-6-1.png" alt="Full-width image"/></p> <p>This last statistic functions kinda like \(R^2\) values in regression except it is measuring how close one is between a null model and the original predictions.</p> <h2 id="extensions">Extensions</h2> <p>One obvious extension is to have arbitrary transformations of the preditive function \(x\beta\). This would be useful in the case where we have something like predictions on a probability space and we want our coefficients to be selected such that they do the best job of predicting on that space rather than the linear predictor space:</p> \[\inf_\beta \sum_i \| \hat{y}_i - h(x \beta_i)\|_p^p + ...\] <p>This may also allow the models to have uses in other applications, which hopefully I will be working on soon!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[My first package is finally live on CRAN]]></summary></entry><entry><title type="html">RcppCGAL is back on CRAN</title><link href="https://ericdunipace.github.io/blog/2023/RcppCGAL-back-on-cran/" rel="alternate" type="text/html" title="RcppCGAL is back on CRAN"/><published>2023-12-08T00:00:00+00:00</published><updated>2023-12-08T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2023/RcppCGAL-back-on-cran</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2023/RcppCGAL-back-on-cran/"><![CDATA[<p>After a bit of a hiatus which saw the package be archived from CRAN due to a bug and their varying machine requirements, the package is back on CRAN.</p> <h2 id="whats-new">What’s new</h2> <p>The header files are now set to a use a fixed version, per CRAN request, and the function <code class="language-plaintext highlighter-rouge">cgal_install()</code> has been deprecated. The reason is that the header files come pre-bundled with the package. One can still use the <code class="language-plaintext highlighter-rouge">CGAL_DIR</code> environmental variable to use one’s own version of CGAL, if so desired. You can also use the function <code class="language-plaintext highlighter-rouge">set_cgal</code> if you’re not comfortable with setting system environment variables.</p> <p>There was also a bug caused by my over zealous cleaning function destroying a CGAL exit template that was needed by a reverse dependency. Thank you to Tyler Morgan-Wall for catching it! He’s been added as a contributor.</p> <h2 id="where-can-you-find-the-package">Where can you find the package?</h2> <p>As always, on the <a href="https://www.github.com/ericdunipace/RcppCGAL">github</a> and from time to time on <a href="https://CRAN.R-project.org/package=RcppCGAL">CRAN</a></p> <h2 id="whats-next">What’s next</h2> <p>Hopefully nothing till the next bug!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[RcppCGAL updated and back on CRAN sitemap: false]]></summary></entry><entry><title type="html">RcppCGAL updated</title><link href="https://ericdunipace.github.io/blog/2023/RcppCGAL-updates/" rel="alternate" type="text/html" title="RcppCGAL updated"/><published>2023-05-29T00:00:00+00:00</published><updated>2023-05-29T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2023/RcppCGAL-updates</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2023/RcppCGAL-updates/"><![CDATA[<p>I’ve done some fairly extensive changes to the <a href="https://github.com/ericdunipace/RcppCGAL">RcppCGAL package</a>, but since this is probably the first time I’ve described it, I should back up a little bit and maybe describe what it does and answer any other anticipated questions you may have.</p> <h2 id="what-does-cgal-stand-for">What does CGAL stand for?</h2> <p>CGAL stands for the Computational Geometry Algorithms Library. You can read more about it <a href="https://www.cgal.org">here</a>.</p> <h2 id="why-have-you-done-this">Why have you done this?</h2> <p>Basically, I did this because I needed the <a href="https://doc.cgal.org/latest/Spatial_sorting/index.html#sechilbert_sorting">Hilbert sorting</a> function from CGAL for the <a href="https://arxiv.org/abs/2012.09999">first paper</a> of my <a href="https://dash.harvard.edu/handle/1/37368342">thesis</a>. It seemed like there could be a need for others to use these header files, so I made a package.</p> <h2 id="what-software-is-it-for">What software is it for?</h2> <p>Anything running in R in need of some C/C++ header files.</p> <h2 id="what-changes-have-you-made">What changes have you made?</h2> <p>The package will try to download the latest header files and will then change the C code to make sure that any messages are printed to the R console and that any errors and stop signals are handled appropriately so that R doesn’t crash.</p> <p>In this specific version, I’ve added a function <code class="language-plaintext highlighter-rouge">cgal_install</code> to download the header files and change the outputs. This will hopefully make it easier to update the header files but also will allow users to install the header files when the automatic download on install doesn’t work.</p> <h2 id="how-do-i-use-it">How do I use it?</h2> <p>Really, this is a bare bones package simply meant to make the process of including the latest CGAL header files into an R package easier. To use it you just need to install the package in the usual manner from R: <code class="language-plaintext highlighter-rouge">install.packages("RcppCGAL")</code>. You can also use the <code class="language-plaintext highlighter-rouge">remotes</code> package to download directly from GitHub. There’s a vignette in the package that also describes how to install the header files.</p> <p>To see what kinds of things are possible with CGAL, I recommend checking out Stéphane Laurent’s packages. He details some of them <a href="https://laustep.github.io/stlahblog/posts/SurfaceReconstruction.html">here</a>.</p> <p>From my own uses, I have the following example in the README file of the GitHub on how to do Hilbert sorting</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// [[Rcpp::depends(RcppCGAL)]]</span>
<span class="c1">// [[Rcpp::depends(BH)]]</span>
<span class="c1">// [[Rcpp::depends(RcppEigen)]]</span>
<span class="c1">// [[Rcpp::plugins(cpp14)]]  </span>

<span class="cp">#include</span> <span class="cpf">&lt;RcppEigen.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;CGAL/basic.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;CGAL/Cartesian_d.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;CGAL/spatial_sort.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;CGAL/Spatial_sort_traits_adapter_d.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;CGAL/boost/iterator/counting_iterator.hpp&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;CGAL/hilbert_sort.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;CGAL/Spatial_sort_traits_adapter_d.h&gt;</span><span class="cp">
</span>
<span class="k">typedef</span> <span class="n">CGAL</span><span class="o">::</span><span class="n">Cartesian_d</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span>           <span class="n">Kernel</span><span class="p">;</span>
<span class="k">typedef</span> <span class="n">Kernel</span><span class="o">::</span><span class="n">Point_d</span>                     <span class="n">Point_d</span><span class="p">;</span>

<span class="k">typedef</span> <span class="n">CGAL</span><span class="o">::</span><span class="n">Spatial_sort_traits_adapter_d</span><span class="o">&lt;</span><span class="n">Kernel</span><span class="p">,</span> <span class="n">Point_d</span><span class="o">*&gt;</span>   <span class="n">Search_traits_d</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">hilbert_sort_cgal_fun</span><span class="p">(</span><span class="k">const</span> <span class="kt">double</span> <span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">int</span> <span class="n">D</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span>  <span class="kt">int</span> <span class="o">*</span> <span class="n">idx</span><span class="p">)</span>
<span class="p">{</span>
  
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Point_d</span><span class="o">&gt;</span> <span class="n">v</span><span class="p">;</span>
  <span class="kt">double</span> <span class="o">*</span> <span class="n">temp</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">double</span><span class="p">[</span><span class="n">D</span><span class="p">];</span>
  
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span> <span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">D</span><span class="p">;</span> <span class="n">d</span> <span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">temp</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">D</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">d</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">v</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">Point_d</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">temp</span><span class="p">,</span> <span class="n">temp</span><span class="o">+</span><span class="n">D</span><span class="p">));</span>
  <span class="p">}</span>
  
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kt">ptrdiff_t</span><span class="o">&gt;</span> <span class="n">temp_index</span><span class="p">;</span>
  <span class="n">temp_index</span><span class="p">.</span><span class="n">reserve</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
  
  <span class="n">std</span><span class="o">::</span><span class="n">copy</span><span class="p">(</span>
    <span class="n">boost</span><span class="o">::</span><span class="n">counting_iterator</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kt">ptrdiff_t</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">boost</span><span class="o">::</span><span class="n">counting_iterator</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kt">ptrdiff_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">size</span><span class="p">()),</span>
    <span class="n">std</span><span class="o">::</span><span class="n">back_inserter</span><span class="p">(</span><span class="n">temp_index</span><span class="p">)</span> <span class="p">);</span>
  
  <span class="n">CGAL</span><span class="o">::</span><span class="n">hilbert_sort</span> <span class="p">(</span><span class="n">temp_index</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span> <span class="n">temp_index</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">Search_traits_d</span><span class="p">(</span> <span class="o">&amp;</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="p">)</span> <span class="p">)</span> <span class="p">;</span>
  
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">idx</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp_index</span><span class="p">[</span><span class="n">n</span><span class="p">];</span>
  <span class="p">}</span>
  
  <span class="k">delete</span> <span class="p">[]</span> <span class="n">temp</span><span class="p">;</span>
  <span class="n">temp</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// [[Rcpp::export]]</span>
<span class="n">Rcpp</span><span class="o">::</span><span class="n">IntegerVector</span> <span class="nf">hilbertSort</span><span class="p">(</span><span class="k">const</span> <span class="n">Eigen</span><span class="o">::</span><span class="n">MatrixXd</span> <span class="o">&amp;</span> <span class="n">A</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">int</span> <span class="n">K</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">rows</span><span class="p">();</span>
  <span class="kt">int</span> <span class="n">N</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">cols</span><span class="p">();</span>
  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">idx</span><span class="p">(</span><span class="n">N</span><span class="p">);</span>
  
  <span class="n">hilbert_sort_cgal_fun</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">);</span>
  <span class="k">return</span><span class="p">(</span><span class="n">Rcpp</span><span class="o">::</span><span class="n">wrap</span><span class="p">(</span><span class="n">idx</span><span class="p">));</span>
<span class="p">}</span>
</code></pre></div></div> <p>Saving this code as <code class="language-plaintext highlighter-rouge">hilbertSort.cpp</code> and sourcing with Rcpp <code class="language-plaintext highlighter-rouge">Rcpp::sourceCpp("hilbertSort.cpp")</code> makes the function <code class="language-plaintext highlighter-rouge">hilbertSort()</code>. Be aware that this example function example assumes that the observations are stored by column rather than by row, that is as the transpose of the usual <code class="language-plaintext highlighter-rouge">R</code> <code class="language-plaintext highlighter-rouge">matrix</code> or <code class="language-plaintext highlighter-rouge">data.frame</code>.</p> <h2 id="what-next">What next?</h2> <p>Hopefully, there won’t be too much more to this package now that it should autoupdate to the latest version of CGAL.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[RcppCGAL has undergone some updates!]]></summary></entry><entry><title type="html">Bug alert in rqPen!</title><link href="https://ericdunipace.github.io/blog/2020/rqpen-bug/" rel="alternate" type="text/html" title="Bug alert in rqPen!"/><published>2020-04-08T00:00:00+00:00</published><updated>2020-04-08T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2020/rqpen-bug</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2020/rqpen-bug/"><![CDATA[<p>I have recently been working with the <code class="language-plaintext highlighter-rouge">rqPen</code> package in R In order to do some penalized median regression (\(L_1\) regression with a group lasso penalty) and have found a bug in the group lasso code.</p> <p>As it is right now, if someone is trying to fit a quantile regression using the SCAD or MCP penalties, the function is not passing the \(\gamma\) tuning parameter (variable a in the code) to the solver.</p> <p>I’ve filed an issue with the maintainer. Just wanted to flag this in case anyone is working with the <code class="language-plaintext highlighter-rouge">rqPen</code> package currently! Will update when/if I hear back.</p> <hr/> <p><strong>Update!:</strong> I also noticed a bug when using the <code class="language-plaintext highlighter-rouge">rq.group.fit</code> function and trying to do a lasso penalty: it doesn’t appear to be actually doing a group penalty, just a regular lasso. I have filed another bug request.</p> <hr/> <p><strong>Update (4/9/2020):</strong> I heard back from the maintainer and he has submitted an update to CRAN regarding the error for the SCAD/MCP penalties. For the group lasso, he says this is by design(!) that it doesn’t offer a group penalty, and is noted in the package help files. I misunderstood this to mean it was using some fancy linear programming method with the \(L_1\) penalty to enforce group sparsity; this belief was incorrect!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I have recently been working with the rqPen package in R In order to do some penalized median regression (\(L_1\) regression with a group lasso penalty) and have found a bug in the group lasso code.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ericdunipace.github.io/assets/img/blog/lady-bug.png"/><media:content medium="image" url="https://ericdunipace.github.io/assets/img/blog/lady-bug.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">The Traveling Salesman Problem: Installing Concorde on Mac Catalina with CPLEX</title><link href="https://ericdunipace.github.io/blog/2020/TSP/" rel="alternate" type="text/html" title="The Traveling Salesman Problem: Installing Concorde on Mac Catalina with CPLEX"/><published>2020-03-16T00:00:00+00:00</published><updated>2020-03-16T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2020/TSP</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2020/TSP/"><![CDATA[<p>Long time no post and, unfortunately for the casual reader, this post is mostly describing how to install Concorde optimizer for the Traveling Salesman Problem (TSP) with Mac OS Catalina using the CPLEX backend. If this means nothing to you, click away!</p> <p>First: CPLEX is a commercial linear programming solver available free for the academic community. I guess the idea is to hook us so that we pay when we leave or convince our organizations to pay for a license when we leave.</p> <p>Second: Concorde is what the <code class="language-plaintext highlighter-rouge">TSP</code> package in <code class="language-plaintext highlighter-rouge">R</code> calls state of the art for getting an exact solution to the Traveling Salesman Problem. The only problem is that one must install it separately and since Concorde was last updated in 2003, some things don’t work quite as easily as we would like. For example, it was made to run with CPLEX 8 but the current CPLEX version on my machine is 12.10! Now, on to the actual installation details.</p> <ol> <li> <p>Download Concorde<br/> Concorde can be downloaded from <a href="math.uwaterloo.ca/tsp/concorde/downloads/codes/src/co031219.tgz">here</a>.</p> </li> <li>Download and Install CPLEX<br/> See the <a href="https://www.ibm.com/products/ilog-cplex-optimization-studio">IBM website</a> for more details</li> <li>Untar the Concorde files<br/> <pre><code class="language-unix">tar xvf co031219.tar
</code></pre> </li> <li>Enter the Concorde directory<br/> <pre><code class="language-unix">cd /path/to/concorde
</code></pre> </li> <li>Create a symlink to the CPLEX files<br/> On my machine, this looks something like:<br/> <pre><code class="language-unix">ln -s /path/to/cplex/include/ilcplex/*.h .
ln -s /path/to/cplex/lib/x86-64_osx/static_pic/libcplex.a .
</code></pre> <p>note: this is done inside the Concorde directory.</p> </li> <li>Add compiler flags<br/> <pre><code class="language-unix">export CFLAGS="-g -O3 -arch x86_64"
</code></pre> </li> <li>Change Concorde files<br/> In “concorde/Makefile.in” change LIBFLAGS to:<br/> <pre><code class="language-unix">LIBFLAGS = @LIBS@ -lpthread
</code></pre> <p>In “concorde/TSP/Makefile.in” change LIBFLAGS to:<br/></p> <pre><code class="language-unix">LIBFLAGS = @LIBS@ -lpthread -ldl
</code></pre> <p>In “concorde/LP/lpcplex8.c” after #undef  CC_CPLEX_DISPLAY add:<br/></p> <pre><code class="language-unix">#ifndef CPX_PARAM_FASTMIP
#define CPX_PARAM_FASTMIP 1017
#endif
</code></pre> <p>In “concorde/TSP/tsp_call.c” at line 479, change the 0 to a 1 (see <a href="https://www.ibm.com/developerworks/community/forums/html/topic?id=96be05f6-fca8-4679-8733-28c034755ffc&amp;permalinkReplyUuid=f7df291c-4717-45c6-b416-a86c69100d35">this post</a> by one of the designers of Concorde)</p> </li> <li>Run configure<br/> Note the host option. It gives a warning saying it doesn’t know what “darwin” is on my machine, but won’t run without it ¯_(ツ)_/¯ <br/> <pre><code class="language-unix">./configure --host=darwin --prefix=/path/to/concorde –with-cplex=/path/to/concorde
</code></pre> </li> <li>Run make<br/> <pre><code class="language-unix">make
</code></pre> </li> <li>If you can run Concorde without an error, you’re done! The easiest way for me is within the TSP R package. You may need to specify the location of the Concorde TSP files like so:<br/> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tsp</span><span class="o">::</span><span class="n">concorde_path</span><span class="p">(</span><span class="n">path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"/path/to/concorde/TSP"</span><span class="p">)</span><span class="w">
</span></code></pre></div> </div> </li> <li>And then check the TSP example:<br/> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">(</span><span class="s2">"USCA312"</span><span class="p">,</span><span class="w"> </span><span class="n">package</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"TSP"</span><span class="p">)</span><span class="w">
</span><span class="n">TSP</span><span class="o">::</span><span class="n">solve_TSP</span><span class="p">(</span><span class="n">USCA312</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"concorde"</span><span class="p">,</span><span class="w"> </span><span class="n">control</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">clo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"-V"</span><span class="p">))</span><span class="w">
</span></code></pre></div> </div> <p>If that works, then you should be all set!</p> </li> </ol> <p>Details from this post come from <a href="leandro-coelho.com/install-and-run-concorde-with-cplex/">this other post</a> by Professor Leandro Coelho and <a href="qmha.wordpress.com/2015/08/20/installing-concorde-on-mac-os-x/">this post</a> by Hà Quang Minh, adapted to what worked on my machine (mid-2019 MacBook Air).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Long time no post and, unfortunately for the casual reader, this post is mostly describing how to install Concorde optimizer for the Traveling Salesman Problem (TSP) with Mac OS Catalina using the CPLEX backend. If this means nothing to you, click away!]]></summary></entry><entry><title type="html">Deep Neural Network Playground</title><link href="https://ericdunipace.github.io/blog/2019/DNN-playground/" rel="alternate" type="text/html" title="Deep Neural Network Playground"/><published>2019-04-04T00:00:00+00:00</published><updated>2019-04-04T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2019/DNN-playground</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2019/DNN-playground/"><![CDATA[<p>I’m trying to simulate some binary data that a Bayesian Deep Neural Network (BDNN?) will do a good job of predicting. This has proven difficult because 1) it’s hard (for me!) to develop a complicated function of the covariates that would make sense to use a BDNN on and 2) to actually fit the BDNN such that it gives good performance.</p> <p>While I was poking around on the internet, I came across <a href="https://playground.tensorflow.org/">this</a> Deep Neural Network Playground from the folks at tensorflow, which I had seen previously in a class. It’s nice because you don’t need to install anything and it has a nice graphical interface to play around with. You can see how adding network layers helps predictions, how adding nodes changes predictions, and can even play around with all the parameters. It also gives a sense to what might charitably be called the “art” of machine learning since you have to tune all of these parameters to get things to work well.</p> <p>Have fun! And hint: decrease the learning rate as you go.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I’m trying to simulate some binary data that a Bayesian Deep Neural Network (BDNN?) will do a good job of predicting. This has proven difficult because 1) it’s hard (for me!) to develop a complicated function of the covariates that would make sense to use a BDNN on and 2) to actually fit the BDNN such that it gives good performance.]]></summary></entry><entry><title type="html">Sometimes in research, you get stuck</title><link href="https://ericdunipace.github.io/blog/2019/research-stuck/" rel="alternate" type="text/html" title="Sometimes in research, you get stuck"/><published>2019-03-14T00:00:00+00:00</published><updated>2019-03-14T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2019/research-stuck</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2019/research-stuck/"><![CDATA[<p>And you feel just like this</p> <p class="figure"><img src="/assets/img/blog/26-fat-rat-sewer.w700.h467.2x.jpg" alt="Full-width image" class="lead" data-width="800" data-height="100"/></p>]]></content><author><name></name></author><summary type="html"><![CDATA[And you feel just like this]]></summary></entry><entry><title type="html">The Bias-Variance Trade-off</title><link href="https://ericdunipace.github.io/blog/2019/bias-variance/" rel="alternate" type="text/html" title="The Bias-Variance Trade-off"/><published>2019-03-04T00:00:00+00:00</published><updated>2019-03-04T00:00:00+00:00</updated><id>https://ericdunipace.github.io/blog/2019/bias-variance</id><content type="html" xml:base="https://ericdunipace.github.io/blog/2019/bias-variance/"><![CDATA[<p>As you get into statistics, you hear people talk about the “bias-variance trade-off” a lot. And when people mention it, they will often start discussing the mean-squared error, which is a function of the bias and variance of an estimator. Then you start looking at formulas. But what does this concept mean intuitively?</p> <p>To answer this, imagine we have some data \(y_1, y_2, y_3,...,y_n\). The first thing we often want to do is calculate the mean, \(\overline{y} = \frac{1}{n} \sum_{i=1}^n y_i\) and variance \(s^2 = \frac{1}{n-1}\sum_{i=1}^n \left(y_i - \overline{y} \right )^2\). We learn in our first statistics class to get the sample mean we divide by \(n\) but to get the sample variance we divide by \(n-1\); I remember this being terribly confusing. To add to the confusion, if we instead divide by \(n\) in our estimate of the sample variance \(\left( \frac{1}{n}\sum_{i=1}^n \left(y_i - \overline{y} \right )^2\right)\), we obtain what is known as the maximum likelihood estimator (MLE) of the variance, which also sounds like a good thing (Maximum! Likelihood!? Sign me up!). We have \(n\) data points, so why don’t we want to divide both the mean and variance by \(n\)?</p> <p>The reason is that if we divide by \(n\) to calculate the variance, our estimate is biased. But will the variance of our estimate also be lower? Yes!</p> <p>If we repeatedly draw datasets of size 1000 from a Normal(0,1) distribution and calculate the variance for each dataset with both methods, we’ll get a distribution of estimates. With this distribution we’ll get a sense of how much each estimator is biased but also the variance of the estimator. This is what is displayed in figure 1 below. You can see the sampling distribution for both methods with the variance of our estimates given on the figure.</p> <p class="figure"><img src="/assets/img/blog/bias_var_1.png" alt="Full-width image" class="lead" data-width="800" data-height="100"/> Figure 1: Distribution of the estimates for the variance of 1000 data points drawn from a Normal(0,1) model.</p> <p>Thus, by introducing some bias we reduce the variance. You can see in figure 1 that this \(n\) vs \(n-1\) doesn’t have much of an effect: the estimate from dividing by \(n-1\) has a variance of about 0.002159, while the estimate of \(\sigma^2\) from dividing by \(n\) has a variance of about 0.002154. Not a huge difference. Similarly, the unbiased estimator (dividing by \(n-1\)) is centered at the correct value of 1 while the MLE estimator (dividing by \(n\)) is centered at 0.999.</p> <p>A case where the trade-off between bias and variance is easier to see arises in a simple linear regression. The variance around the regression line is usually calculated as \(\hat{\sigma^2}_{\text{unbiased}} = \frac{1}{n-p}\sum_{i=1}^n \left(y_i - \hat{y_i} \right )^2,\) where \(\hat{y_i}\) is the fitted value for observation \(i\) and \(p\) is the number of parameters estimated in the regression. However, the MLE for the same quantity is \(\hat{\sigma^2}_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{y_i} \right )^2\). What effect does using the MLE instead of the unbiased estimator have on our estimates when we have a regression with, say, 500 parameters and 1000 observations?</p> <p>Let’s assume the variance around the regression line is actually 1. When we repeatedly draw datasets of size 1000 and calculate the variance around the line with both methods, we again get a distribution of estimates and we see the following:</p> <p class="figure"><img src="/assets/img/blog/bias_var_2.png" alt="Full-width image" class="lead" data-width="800" data-height="100"/> Figure 2: Estimates for the variance around regression line when the true variance is 1. \(n = 1000\) and \(p = 500\).</p> <p>The MLE does have lower variance—75% smaller—but it is quite biased! The true variance is 1 but the MLE is centered at 0.5. This is an extreme case, since as you get more observations in each dataset relative to the number of parameters, both the MLE and the unbiased estimator look basically the same. But it clearly illustrates the bias-variance trade off.</p> <p>R Code to replicate this post is below:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#bias variance</span><span class="w">
</span><span class="n">rm</span><span class="p">(</span><span class="n">list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ls</span><span class="p">())</span><span class="w">

</span><span class="n">require</span><span class="p">(</span><span class="n">mvtnorm</span><span class="p">)</span><span class="w">
</span><span class="n">require</span><span class="p">(</span><span class="n">doRNG</span><span class="p">)</span><span class="w">
</span><span class="n">require</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">
</span><span class="n">require</span><span class="p">(</span><span class="n">ggridges</span><span class="p">)</span><span class="w">

</span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">parallel</span><span class="o">::</span><span class="n">detectCores</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
	</span><span class="n">library</span><span class="p">(</span><span class="n">parallel</span><span class="p">)</span><span class="w">
	</span><span class="n">ncores</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">detectCores</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">2</span><span class="w">
	</span><span class="n">library</span><span class="p">(</span><span class="n">doParallel</span><span class="p">)</span><span class="w">
	</span><span class="n">library</span><span class="p">(</span><span class="n">foreach</span><span class="p">)</span><span class="w">
	</span><span class="n">library</span><span class="p">(</span><span class="n">doRNG</span><span class="p">)</span><span class="w">
	</span><span class="n">registerDoParallel</span><span class="p">(</span><span class="n">cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ncores</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">nexperiment</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="c1"># number of replications</span><span class="w">
</span><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span><span class="w"> </span><span class="c1"># number of observations</span><span class="w">
</span><span class="n">p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">500</span><span class="w"> </span><span class="c1"># number of parameters</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">11</span><span class="p">)</span><span class="w">

</span><span class="c1">#set regression coefficients</span><span class="w">
</span><span class="n">beta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="w">

</span><span class="n">output</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">foreach</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">nexperiment</span><span class="p">)</span><span class="w"> </span><span class="o">%dorng%</span><span class="w"> </span><span class="p">{</span><span class="w">
	</span><span class="c1"># data</span><span class="w">
	</span><span class="n">X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rmvnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diag</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">))</span><span class="w">
	</span><span class="n">Y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w">

	</span><span class="c1"># model</span><span class="w">
	</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">Y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="c1"># add 0 to make sure R does not fit an intercept</span><span class="w">
	</span><span class="n">bhat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fit</span><span class="o">$$</span><span class="n">coef</span><span class="w">

	</span><span class="c1">#variance</span><span class="w">
	</span><span class="n">sigma_mle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">((</span><span class="n">Y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">bhat</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="w">
	</span><span class="n">sigma_unb</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">((</span><span class="n">Y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">bhat</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w">
	
	</span><span class="n">Z</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">Y</span><span class="o">/</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">beta</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="m">+1</span><span class="p">)</span><span class="w">
	
	</span><span class="n">sigma_marg_unb</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">((</span><span class="n">Z</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">Z</span><span class="p">))</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="m">-1</span><span class="p">)</span><span class="w">
	</span><span class="n">sigma_marg_mle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">((</span><span class="n">Z</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">Z</span><span class="p">))</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="w">

	</span><span class="c1">#output</span><span class="w">
	</span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">conditional</span><span class="w"> </span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="n">mle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigma_mle</span><span class="p">,</span><span class="w"> </span><span class="n">unbiased</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigma_unb</span><span class="p">),</span><span class="w">
	</span><span class="n">marginal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">mle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigma_marg_mle</span><span class="p">,</span><span class="w"> </span><span class="n">unbiased</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigma_marg_unb</span><span class="p">)))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span><span class="c1"># Marginal Variance</span><span class="w">
</span><span class="n">sigma_marg_mle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="w"> </span><span class="n">res</span><span class="o">$</span><span class="n">marginal</span><span class="o">$</span><span class="n">mle</span><span class="p">)</span><span class="w">
</span><span class="n">sigma_marg_unb</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="w"> </span><span class="n">res</span><span class="o">$</span><span class="n">marginal</span><span class="o">$</span><span class="n">unbiased</span><span class="p">)</span><span class="w">

</span><span class="n">sigma_df_marg</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">sigma_marg_mle</span><span class="p">,</span><span class="w"> </span><span class="n">sigma_marg_unb</span><span class="p">),</span><span class="w"> 
  </span><span class="n">Method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="s2">"MLE"</span><span class="p">,</span><span class="w"> </span><span class="n">nexperiment</span><span class="p">),</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="s2">"Unbiased"</span><span class="p">,</span><span class="w"> </span><span class="n">nexperiment</span><span class="p">)))</span><span class="w">
	
</span><span class="n">marg_vars</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s2">"Variance = "</span><span class="p">,</span><span class="n">format</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">sigma_marg_mle</span><span class="p">),</span><span class="w"> 
  </span><span class="n">var</span><span class="p">(</span><span class="n">sigma_marg_unb</span><span class="p">)),</span><span class="n">digits</span><span class="o">=</span><span class="m">4</span><span class="p">))</span><span class="w">
  </span><span class="n">E_marg</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">sigma_marg_mle</span><span class="p">),</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">sigma_marg_unb</span><span class="p">))</span><span class="w">

</span><span class="n">ggplot</span><span class="p">(</span><span class="n">sigma_df_marg</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigma</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Method</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">geom_density_ridges</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Sigma^2"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">annotate</span><span class="p">(</span><span class="n">geom</span><span class="o">=</span><span class="s2">"text"</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">E_marg</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1.5</span><span class="p">,</span><span class="m">2.9</span><span class="p">),</span><span class="w"> </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">marg_vars</span><span class="p">)</span><span class="w">


</span><span class="c1"># Regression Variance</span><span class="w">
</span><span class="n">sigma_hat_mle</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="w"> </span><span class="n">res</span><span class="o">$</span><span class="n">conditional</span><span class="o">$</span><span class="n">mle</span><span class="p">)</span><span class="w">
</span><span class="n">sigma_hat_unb</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="w"> </span><span class="n">res</span><span class="o">$</span><span class="n">conditional</span><span class="o">$</span><span class="n">unbiased</span><span class="p">)</span><span class="w">

</span><span class="n">vars</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s2">"Variance = "</span><span class="p">,</span><span class="n">format</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">var</span><span class="p">(</span><span class="n">sigma_hat_mle</span><span class="p">),</span><span class="w">
  </span><span class="n">var</span><span class="p">(</span><span class="n">sigma_hat_unb</span><span class="p">)),</span><span class="n">digits</span><span class="o">=</span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">E_sigma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">sigma_hat_mle</span><span class="p">),</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">sigma_hat_unb</span><span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w">


</span><span class="n">sigma_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">sigma_hat_mle</span><span class="p">,</span><span class="w"> </span><span class="n">sigma_hat_unb</span><span class="p">),</span><span class="w"> 
  </span><span class="n">Method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="s2">"MLE"</span><span class="p">,</span><span class="w"> </span><span class="n">nexperiment</span><span class="p">),</span><span class="w"> 
	</span><span class="nf">rep</span><span class="p">(</span><span class="s2">"Unbiased"</span><span class="p">,</span><span class="w"> </span><span class="n">nexperiment</span><span class="p">)))</span><span class="w">

</span><span class="n">ggplot</span><span class="p">(</span><span class="n">sigma_df</span><span class="p">,</span><span class="w"> </span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigma</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Method</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w"> 
  </span><span class="n">geom_density_ridges</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">xlab</span><span class="p">(</span><span class="s2">"Sigma^2"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">annotate</span><span class="p">(</span><span class="n">geom</span><span class="o">=</span><span class="s2">"text"</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">E_sigma</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0.9</span><span class="p">,</span><span class="m">1.9</span><span class="p">),</span><span class="w"> </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vars</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[As you get into statistics, you hear people talk about the “bias-variance trade-off” a lot. And when people mention it, they will often start discussing the mean-squared error, which is a function of the bias and variance of an estimator. Then you start looking at formulas. But what does this concept mean intuitively?]]></summary></entry></feed>